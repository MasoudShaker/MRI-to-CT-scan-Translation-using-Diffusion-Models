{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from utils import resize, convert_to_tensor, norm, fill_nan_reshape, pre_process\n",
    "from utils import get_ssim, get_psnr, get_mse, get_mae, save_best_samples, save_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_gray_level = T.Compose([\n",
    "    T.Lambda(lambda t: t * 0.2)\n",
    "])\n",
    "\n",
    "horizontal_flip = T.Compose([\n",
    "    T.functional.hflip\n",
    "])\n",
    "\n",
    "vertical_flip = T.Compose([\n",
    "    T.functional.vflip\n",
    "])\n",
    "\n",
    "rotate_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=45))\n",
    "])\n",
    "\n",
    "rotate_minus_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=-45))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CERMEP dataset\n",
    "data_path = '../Brain_tumor_MR_CT.npz'\n",
    "data_loaded = np.load(data_path)\n",
    "\n",
    "data_pairs = data_loaded[\"arr_0\"]\n",
    "\n",
    "data_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "train_percent = 0.9\n",
    "\n",
    "n_samples = data_pairs.shape[0]\n",
    "train_size = int(train_percent*n_samples)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "\n",
    "test_indices = []\n",
    "\n",
    "# loop until the list has the desired size\n",
    "while len(test_indices) < test_size:\n",
    "  # generate a random number between 1 and 100\n",
    "  num = random.randint(0, n_samples-1)\n",
    "  # check if the number is already in the list\n",
    "  if num not in test_indices:\n",
    "    # add the number to the list\n",
    "    test_indices.append(num)\n",
    "\n",
    "all_indices = list(range(n_samples))\n",
    "\n",
    "train_indices_filter = filter(lambda i: i not in test_indices, all_indices)\n",
    "\n",
    "train_indices = list(train_indices_filter)\n",
    "\n",
    "train_data = data_pairs[train_indices]\n",
    "test_data = data_pairs[test_indices]\n",
    "\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_train = []\n",
    "ct_train = []\n",
    "\n",
    "mr_test = []\n",
    "ct_test = []\n",
    "\n",
    "\n",
    "for train_pair in train_data:\n",
    "\n",
    "    mr = train_pair[0]\n",
    "    mr = fill_nan_reshape(mr, img_size)\n",
    "\n",
    "    ct = train_pair[1]\n",
    "    ct = fill_nan_reshape(ct, img_size)\n",
    "\n",
    "    mr_train.append(mr)\n",
    "    ct_train.append(ct)\n",
    "\n",
    "\n",
    "for test_pair in test_data:\n",
    "\n",
    "    mr = test_pair[0]\n",
    "    mr = fill_nan_reshape(mr, img_size)\n",
    "\n",
    "    ct = test_pair[1]\n",
    "    ct = fill_nan_reshape(ct, img_size)\n",
    "\n",
    "    mr_test.append(mr)\n",
    "    ct_test.append(ct)\n",
    "\n",
    "\n",
    "mr_train = np.array(mr_train)\n",
    "mr_test = np.array(mr_test)\n",
    "ct_train = np.array(ct_train)\n",
    "ct_test = np.array(ct_test)\n",
    "\n",
    "mr_train.shape, mr_test.shape, ct_train.shape, ct_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation for training data\n",
    "\n",
    "# we want to do 5 augmentations\n",
    "# so our training data size will be 6 times bigger\n",
    "train_size_augmented = train_size * 6\n",
    "\n",
    "mr_train_augmented = [None] * train_size_augmented\n",
    "ct_train_augmented = [None] * train_size_augmented\n",
    "\n",
    "\n",
    "# augment training data\n",
    "for i in range(train_size):\n",
    "\n",
    "  j = i * 6\n",
    "\n",
    "  mr = convert_to_tensor(mr_train[i])\n",
    "\n",
    "  mr_train_augmented[j] = mr\n",
    "  mr_train_augmented[j+1] = change_gray_level(mr)\n",
    "  mr_train_augmented[j+2] = horizontal_flip(mr)\n",
    "  mr_train_augmented[j+3] = vertical_flip(mr)\n",
    "  mr_train_augmented[j+4] = rotate_45(mr)\n",
    "  mr_train_augmented[j+5] = rotate_minus_45(mr)\n",
    "\n",
    "  ct = convert_to_tensor(ct_train[i])\n",
    "\n",
    "  ct_train_augmented[j] = ct\n",
    "  ct_train_augmented[j+1] = change_gray_level(ct)\n",
    "  ct_train_augmented[j+2] = horizontal_flip(ct)\n",
    "  ct_train_augmented[j+3] = vertical_flip(ct)\n",
    "  ct_train_augmented[j+4] = rotate_45(ct)\n",
    "  ct_train_augmented[j+5] = rotate_minus_45(ct)\n",
    "\n",
    "\n",
    "# convert train samples to numpy array and normalize them\n",
    "for i in range(train_size_augmented):\n",
    "\n",
    "  mr_train_augmented[i] = np.array(mr_train_augmented[i].squeeze())\n",
    "  ct_train_augmented[i] = np.array(ct_train_augmented[i].squeeze())\n",
    "\n",
    "  mr_train_augmented[i] = norm(mr_train_augmented[i])\n",
    "  ct_train_augmented[i] = norm(ct_train_augmented[i])\n",
    "\n",
    "\n",
    "# convert lists of mr and ct (train data) to numpy arrays\n",
    "mr_train_augmented = np.array(mr_train_augmented)\n",
    "ct_train_augmented = np.array(ct_train_augmented)\n",
    "\n",
    "\n",
    "# normalize test samples\n",
    "for i in range(test_size):\n",
    "  mr_test[i] = norm(mr_test[i])\n",
    "  ct_test[i] = norm(ct_test[i])\n",
    "\n",
    "\n",
    "print('train images shape:', mr_train_augmented.shape)\n",
    "print('test images shape:', mr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = True\n",
    "\n",
    "# for sample in mr_train_augmented:\n",
    "for sample in mr_test:\n",
    "    if sample.max() != 1.0:\n",
    "        normalized = False\n",
    "\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Datasets with augmentation \n",
    "import h5py\n",
    "\n",
    "# creating hdf5 data from numpy arrays\n",
    "with h5py.File('mr_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_train_augmented)\n",
    "\n",
    "with h5py.File('ct_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_train_augmented)\n",
    "\n",
    "with h5py.File('mr_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_test)\n",
    "\n",
    "with h5py.File('ct_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Datasets without augmentation \n",
    "\n",
    "import h5py\n",
    "\n",
    "# creating hdf5 data from numpy arrays\n",
    "with h5py.File('mr_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_train)\n",
    "\n",
    "with h5py.File('ct_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_train)\n",
    "\n",
    "with h5py.File('mr_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_test)\n",
    "\n",
    "with h5py.File('ct_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mr_train_resized.hdf5'\n",
    "f = h5py.File(path,'r')\n",
    "load_data = f['data']\n",
    "load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py DDPM folder\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion import GaussianDiffusionTrainer, GaussianDiffusionSampler\n",
    "from model import UNet\n",
    "from dataset import Train_Data, Test_Data\n",
    "\n",
    "\n",
    "train = True\n",
    "\n",
    "# UNet\n",
    "ch = 64\n",
    "ch_mult = [1, 2, 2, 4, 4]\n",
    "attn = [1]\n",
    "num_res_blocks = 2\n",
    "dropout = 0.\n",
    "\n",
    "# Gaussian Diffusion\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Training\n",
    "lr = 1e-4\n",
    "grad_clip = 1.\n",
    "img_size = 128\n",
    "batch_size = 2\n",
    "num_workers = 1\n",
    "ema_decay = 0.9999\n",
    "\n",
    "sample_size = 1\n",
    "\n",
    "min_epoch = 100\n",
    "max_epoch = 110\n",
    "n_prev_epochs = 20\n",
    "\n",
    "epoch_mean_loss = max_epoch * [None]\n",
    "# Logging & Sampling\n",
    "DIREC = f'ddpm-unet_n-train-samples_{train_size_augmented}_n-test-samples_{test_size}_batch-size_{batch_size}_T_{T}_img-size_{img_size}_atlas_data'\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def ema(source, target, decay):\n",
    "    source_dict = source.state_dict()\n",
    "    target_dict = target.state_dict()\n",
    "    for key in source_dict.keys():\n",
    "        target_dict[key].data.copy_(\n",
    "            target_dict[key].data * decay +\n",
    "            source_dict[key].data * (1 - decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # dataset\n",
    "    train_data = Train_Data()\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers,\n",
    "                             pin_memory=True, shuffle=True)\n",
    "\n",
    "    # model setup\n",
    "    net_model = UNet(\n",
    "        T=T, ch=ch, ch_mult=ch_mult, attn=attn,\n",
    "        num_res_blocks=num_res_blocks, dropout=dropout)\n",
    "    ema_model = copy.deepcopy(net_model)\n",
    "\n",
    "    net_model.to(device)\n",
    "    ema_model.to(device)\n",
    "\n",
    "\n",
    "    optim = torch.optim.Adam(net_model.parameters(), lr=lr)\n",
    "\n",
    "    trainer = GaussianDiffusionTrainer(\n",
    "        net_model, beta_1, beta_T, T).to(device)\n",
    "\n",
    "\n",
    "    # show model size\n",
    "    model_size = 0\n",
    "    for param in net_model.parameters():\n",
    "        model_size += param.data.nelement()\n",
    "    print('Model params: %.2f M' % (model_size / 1024 / 1024))\n",
    "\n",
    "\n",
    "    if not os.path.exists('current experiment'):\n",
    "        os.makedirs('current experiment')\n",
    "\n",
    "    if not os.path.exists('./current experiment/Saved_model'):\n",
    "        os.makedirs('./current experiment/Saved_model')\n",
    "\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tmp_tr_loss = 0\n",
    "            tr_sample = 0\n",
    "            net_model.train()\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "                # train\n",
    "                optim.zero_grad()\n",
    "                condition = data.to(device)\n",
    "                x_0 = target.to(device)\n",
    "\n",
    "                loss = trainer(x_0, condition)\n",
    "                tmp_tr_loss += loss.item()\n",
    "                tr_sample += len(data)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    net_model.parameters(), grad_clip)\n",
    "                optim.step()\n",
    "                ema(net_model, ema_model, ema_decay)\n",
    "\n",
    "                tepoch.set_postfix({'Loss': loss.item()})\n",
    "        \n",
    "        mean_loss = tmp_tr_loss / tr_sample\n",
    "        print('mean loss', mean_loss)\n",
    "\n",
    "        epoch_mean_loss[epoch] = mean_loss\n",
    "        \n",
    "        if epoch+1 > min_epoch:\n",
    "          prev_mean_loss = 0\n",
    "          \n",
    "          for i in range(n_prev_epochs):\n",
    "            prev_mean_loss += epoch_mean_loss[epoch - (i+1)]\n",
    "\n",
    "          prev_mean_loss /= n_prev_epochs\n",
    "          \n",
    "          if mean_loss > (prev_mean_loss - 0.01*prev_mean_loss):\n",
    "            break        \n",
    "\n",
    "    torch.save(ema_model.state_dict(), f'./current experiment/Saved_model/ddpm-unet_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch_num = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    if not os.path.exists('./current experiment/Train_Output/' + DIREC):\n",
    "        os.makedirs('./current experiment/Train_Output/' + DIREC)\n",
    "\n",
    "    if not os.path.exists('./current experiment/diff_results'):\n",
    "        os.makedirs('./current experiment/diff_results')\n",
    "\n",
    "    ddpm_sum_time = 0\n",
    "    \n",
    "    test_data = Test_Data()\n",
    "    test_loader = DataLoader(test_data, batch_size=sample_size, num_workers=num_workers,\n",
    "                                pin_memory=True, shuffle=False)\n",
    "\n",
    "    net_model = UNet(\n",
    "    T=T, ch=ch, ch_mult=ch_mult, attn=attn,\n",
    "    num_res_blocks=num_res_blocks, dropout=dropout)\n",
    "    \n",
    "    ema_model = copy.deepcopy(net_model)\n",
    "\n",
    "    model_path = f'./current experiment/Saved_model/ddpm-unet_epoch_{last_epoch_num}.pt'\n",
    "\n",
    "    ema_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    ema_sampler = GaussianDiffusionSampler(\n",
    "    ema_model, beta_1, beta_T, T, img_size).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "                x_T = torch.randn(sample_size, 1, img_size, img_size)\n",
    "                x_T = x_T.to(device)\n",
    "\n",
    "                condition = data.to(device)\n",
    "\n",
    "                tic_ddpm = time.time()\n",
    "                x_0 = ema_sampler(x_T, condition)\n",
    "                toc_ddpm = time.time()\n",
    "\n",
    "                time_interval_ddpm = toc_ddpm - tic_ddpm\n",
    "                ddpm_sum_time += time_interval_ddpm\n",
    "                \n",
    "                ddpm_out = x_0[-1]\n",
    "                ddpm_out = np.array(ddpm_out.cpu())\n",
    "\n",
    "                save_path = f'./current experiment/diff_results/x0_number_{idx+1}_epoch_{last_epoch_num}.npy'\n",
    "                np.save(save_path, ddpm_out)\n",
    "\n",
    "                fig = plt.figure()\n",
    "                fig.set_figheight(8)\n",
    "                fig.set_figwidth(28)\n",
    "                spec = gridspec.GridSpec(ncols=7, nrows=2,\n",
    "                        width_ratios=[1,1,1,1,1,1,1], wspace=0.01,\n",
    "                        hspace=0.01, height_ratios=[1,1],left=0,right=1,top=1,bottom=0)\n",
    "\n",
    "                img = data[0].data.squeeze()\n",
    "                ax = fig.add_subplot(spec[0])\n",
    "                ax.imshow(img, cmap='gray', vmin=0,vmax=1)\n",
    "                ax.axis('off')\n",
    "\n",
    "                count = 1\n",
    "                for kk in range(5): # x_0 [5,b,1,h,w]\n",
    "                    imgs = x_0[kk] # imgs [b,1,h,w]\n",
    "                    img = imgs[0].data.squeeze().cpu()\n",
    "                    ax = fig.add_subplot(spec[count])\n",
    "                    ax.imshow(img, cmap='gray', vmin=0,vmax=1)\n",
    "                    ax.axis('off')\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "                img = target[0].data.squeeze().cpu()\n",
    "                ax = fig.add_subplot(spec[6])\n",
    "                ax.imshow(img, cmap='gray', vmin=0,vmax=1)\n",
    "                ax.axis('off')\n",
    "\n",
    "                plt.savefig('./current experiment/Train_Output/'+ DIREC + '/sample_' + str(idx) + '.png',\n",
    "                            bbox_inches='tight', pad_inches=0)\n",
    "                plt.close(fig)\n",
    "\n",
    "    return ddpm_sum_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_sum_time = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_avg_time = ddpm_sum_time / test_size\n",
    "ddpm_avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch_num = 102\n",
    "DIREC = f'ddpm-unet_n-train-samples_{train_size_augmented}_n-test-samples_{test_size}_batch-size_{batch_size}_T_{T}_img-size_{img_size}_atlas_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_outs = [None] * test_size\n",
    "\n",
    "for i in range(test_size):\n",
    "  path = f'./current experiment/diff_results/x0_number_{i+1}_epoch_{last_epoch_num}.npy'\n",
    "  diff_out = np.load(path)\n",
    "  diff_outs[i] = pre_process(diff_out, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [None] * test_size\n",
    "\n",
    "for i in range(test_size):\n",
    "  ct_sample = ct_test[i]\n",
    "  targets[i] = pre_process(ct_sample, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save average and best values of ssim, psnr, mse, mae\n",
    "# save samples that have best values of ssim, psnr, mse, mae in seperate folders\n",
    "max_ssim, argmax_ssim, avg_ssim = get_ssim(diff_outs, targets)\n",
    "max_psnr, argmax_psnr, avg_psnr = get_psnr(diff_outs, targets)\n",
    "\n",
    "min_mse, argmin_mse, avg_mse = get_mse(diff_outs, targets)\n",
    "min_mae, argmin_mae, avg_mae = get_mae(diff_outs, targets)\n",
    "\n",
    "sampler_type = 'ddpm'\n",
    "\n",
    "save_best_samples(sampler_type, DIREC, diff_outs, targets, argmax_ssim, argmax_psnr, argmin_mse, argmin_mae)\n",
    "save_metrics(sampler_type, ddpm_avg_time, avg_ssim, avg_psnr, avg_mse, avg_mae, max_ssim, max_psnr, min_mse, min_mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
