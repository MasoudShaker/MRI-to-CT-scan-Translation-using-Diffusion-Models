{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/iplab/Desktop/Shaker/Brain tumor MRI and CT scan/data(processed)'\n",
    "\n",
    "# read mr and ct datasets\n",
    "MR_train_address = os.path.join(base_dir, 'train_input.npy')\n",
    "CT_train_address = os.path.join(base_dir, 'train_output.npy')\n",
    "\n",
    "MR_val_address = os.path.join(base_dir, 'val_input.npy')\n",
    "CT_val_address = os.path.join(base_dir, 'val_output.npy')\n",
    "\n",
    "MR_test_address = os.path.join(base_dir, 'test_input.npy')\n",
    "CT_test_address = os.path.join(base_dir, 'test_output.npy')\n",
    "\n",
    "mr_train = np.load(MR_train_address)\n",
    "ct_train = np.load(CT_train_address)\n",
    "\n",
    "mr_val = np.load(MR_val_address)\n",
    "ct_val = np.load(CT_val_address)\n",
    "\n",
    "mr_test = np.load(MR_test_address)\n",
    "ct_test = np.load(CT_test_address)\n",
    "\n",
    "ct_train.shape, ct_val.shape, ct_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes a 1d numpy array to an arbitrary size\n",
    "def resize(img, size):\n",
    "\n",
    "  img = img.astype('float32')\n",
    "  img = torch.tensor(img)\n",
    "  img = img.unsqueeze(0)\n",
    "\n",
    "  transform = T.Resize(size)\n",
    "  resized_img = transform(img)\n",
    "\n",
    "  # resized_img = np.array(resized_img)\n",
    "\n",
    "  return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_gray_level = T.Compose([\n",
    "    T.Lambda(lambda t: t * 0.2)\n",
    "])\n",
    "\n",
    "horizontal_flip = T.Compose([\n",
    "    T.functional.hflip\n",
    "])\n",
    "\n",
    "vertical_flip = T.Compose([\n",
    "    T.functional.vflip\n",
    "])\n",
    "\n",
    "rotate_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=45))\n",
    "])\n",
    "\n",
    "rotate_minus_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=-45))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = ct_train.shape[0]\n",
    "n_val_samples = ct_val.shape[0]\n",
    "n_test_samples = ct_test.shape[0]\n",
    "\n",
    "# remove 90 samples from test data and add it to train data\n",
    "n_add_from_test_to_train = 69\n",
    "\n",
    "n_train_new = n_train_samples + n_val_samples + n_add_from_test_to_train\n",
    "n_test_new = n_test_samples - n_add_from_test_to_train\n",
    "\n",
    "# data augmentation\n",
    "# add 5 varient of each sample\n",
    "# so our train dataset will be 6 times bigger\n",
    "\n",
    "n_train_new = n_train_new * 6\n",
    "# n_train_new = n_train_new * 3\n",
    "\n",
    "mr_train_resized = [None] * n_train_new\n",
    "ct_train_resized = [None] * n_train_new\n",
    "\n",
    "mr_test_resized = [None] * n_test_new\n",
    "ct_test_resized = [None] * n_test_new\n",
    "\n",
    "# train samples with augmentation\n",
    "for i in range(n_train_samples):\n",
    "\n",
    "  j = i * 6\n",
    "  # j = i*3\n",
    "  # j = i\n",
    "\n",
    "  resized_mr = resize(mr_train[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_train[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "\n",
    "# validation samples with augmentation\n",
    "for i in range(n_val_samples):\n",
    "\n",
    "  j = i*6 + n_train_samples*6\n",
    "  # j = i*3 + n_train_samples*3\n",
    "  # j = i + n_train_samples\n",
    "\n",
    "  resized_mr = resize(mr_val[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_val[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "\n",
    "# part of test samples with augmentation\n",
    "\n",
    "for i in range(n_add_from_test_to_train):\n",
    "\n",
    "  j = i*6 + n_train_samples*6 + n_val_samples*6\n",
    "  # j = i*3 + n_train_samples*3 + n_val_samples*3\n",
    "  # j = i + n_train_samples + n_val_samples\n",
    "\n",
    "  resized_mr = resize(mr_test[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_test[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "# test samples\n",
    "for i in range(n_test_new):\n",
    "\n",
    "  j = i + n_add_from_test_to_train\n",
    "\n",
    "  mr_test_resized[i] = resize(mr_test[j], 128)\n",
    "  ct_test_resized[i] = resize(ct_test[j], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test samples to numpy array\n",
    "for i in range(n_train_new):\n",
    "  mr_train_resized[i] = np.array(mr_train_resized[i].squeeze())\n",
    "  ct_train_resized[i] = np.array(ct_train_resized[i].squeeze())\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  mr_test_resized[i] = np.array(mr_test_resized[i].squeeze())\n",
    "  ct_test_resized[i] = np.array(ct_test_resized[i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists of mr and ct to numpy arrays\n",
    "mr_train_resized = np.array(mr_train_resized)\n",
    "\n",
    "ct_train_resized = np.array(ct_train_resized)\n",
    "\n",
    "mr_test_resized = np.array(mr_test_resized)\n",
    "\n",
    "ct_test_resized = np.array(ct_test_resized)\n",
    "\n",
    "\n",
    "print('train images shape:', mr_train_resized.shape)\n",
    "print('test images shape:', mr_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# creating hdf5 data from numpy arrays\n",
    "with h5py.File('mr_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_train_resized)\n",
    "\n",
    "with h5py.File('ct_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_train_resized)\n",
    "\n",
    "with h5py.File('mr_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_test_resized)\n",
    "\n",
    "with h5py.File('ct_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_test_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mr_train_resized.hdf5'\n",
    "f = h5py.File(path,'r')\n",
    "load_data = f['data']\n",
    "load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import os\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "# from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Train_Data, Test_Data\n",
    "from diffusion import ode_sampler, euler_sampler, pc_sampler\n",
    "from model import UNet\n",
    "\n",
    "\n",
    "train = True\n",
    "continue_train = False\n",
    "\n",
    "# UNet\n",
    "ch = 64\n",
    "ch_mult = [1, 2, 2, 4, 4]\n",
    "attn = [1]\n",
    "num_res_blocks = 2\n",
    "dropout = 0.\n",
    "\n",
    "# Gaussian Diffusion\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Training\n",
    "lr = 1e-4\n",
    "grad_clip = 1.\n",
    "img_size = 128\n",
    "batch_size = 2\n",
    "num_workers = 1\n",
    "ema_decay = 0.9999\n",
    "\n",
    "sample_size = 1\n",
    "\n",
    "min_epoch = 100\n",
    "max_epoch = 1000\n",
    "\n",
    "epoch_mean_loss = max_epoch * [None]\n",
    "n_prev_epochs = 20\n",
    "\n",
    "DIREC = f'score-unet_min-epoch_{min_epoch}_n-train-samples_{n_train_new}n-test-samples_{n_test_new}_batch-size_{batch_size}_T_{T}_img-size_{img_size}_data_augmentation_all'\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    od_sum_time = 0\n",
    "    eu_sum_time = 0\n",
    "    pc_sum_time = 0\n",
    "    last_epoch_num = 0\n",
    "\n",
    "    sigma = 25.\n",
    "    marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma) # construc function without parameters\n",
    "    diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma) # construc function without parameters\n",
    "\n",
    "    # dataset\n",
    "    train_data = Train_Data()\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers,\n",
    "                             pin_memory=True, shuffle=True)\n",
    "    test_data = Test_Data()\n",
    "    test_loader = DataLoader(test_data, batch_size=sample_size, num_workers=num_workers,\n",
    "                             pin_memory=True, shuffle=False)\n",
    "\n",
    "    # model setup\n",
    "    score_model = UNet(T=T, ch=ch, ch_mult=ch_mult, attn=attn,\n",
    "                       num_res_blocks=num_res_blocks, dropout=dropout,\n",
    "                       marginal_prob_std=marginal_prob_std_fn).to(device)\n",
    "\n",
    "    ema_model = EMA(score_model).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "    # sampler setup\n",
    "    sampler_od = ode_sampler\n",
    "    sampler_eu = euler_sampler\n",
    "    sampler_pc = pc_sampler\n",
    "\n",
    "    # show model size\n",
    "    model_size = 0\n",
    "    for param in score_model.parameters():\n",
    "        model_size += param.data.nelement()\n",
    "    print('Model params: %.2f M' % (model_size / 1024 / 1024))\n",
    "\n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./Save/' + DIREC + '/model_latest.pkl')\n",
    "        score_model.load_state_dict(checkpoint['score_model'])\n",
    "        ema_model.load_state_dict(checkpoint['ema_model'])\n",
    "        optim.load_state_dict(checkpoint['optim'])\n",
    "        restore_epoch = checkpoint['epoch']\n",
    "        print('Finish loading model')\n",
    "    else:\n",
    "        restore_epoch = 0\n",
    "\n",
    "    if not os.path.exists('current experiment'):\n",
    "        os.makedirs('current experiment')\n",
    "\n",
    "    tr_ls = []\n",
    "    if continue_train:\n",
    "        readmat = sio.loadmat('./Loss/' + DIREC)\n",
    "        load_tr_ls = readmat['loss']\n",
    "        for i in range(restore_epoch):\n",
    "            tr_ls.append(load_tr_ls[0][i])\n",
    "        print('Finish loading loss!')\n",
    "\n",
    "\n",
    "    if not os.path.exists('./current experiment/diff_results'):\n",
    "        os.makedirs('./current experiment/diff_results')\n",
    "\n",
    "    last_epoch = False\n",
    "\n",
    "    for epoch in range(restore_epoch, max_epoch):\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tmp_tr_loss = 0\n",
    "            tr_sample = 0\n",
    "            score_model.train()\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "                # train\n",
    "                condition = data.to(device)\n",
    "                x_0 = target.to(device)\n",
    "\n",
    "                loss = loss_fn(score_model, condition, x_0, marginal_prob_std_fn)\n",
    "\n",
    "                tmp_tr_loss += loss.item()\n",
    "                tr_sample += len(data)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                ema_model.update(score_model)\n",
    "\n",
    "                tepoch.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        mean_loss = tmp_tr_loss / tr_sample\n",
    "        print('mean loss:', mean_loss)\n",
    "\n",
    "        epoch_mean_loss[epoch] = mean_loss\n",
    "        \n",
    "        if epoch+1 > min_epoch:\n",
    "          prev_mean_loss = 0\n",
    "          \n",
    "          for i in range(n_prev_epochs):\n",
    "            prev_mean_loss += epoch_mean_loss[epoch - (i+1)]\n",
    "\n",
    "          prev_mean_loss /= n_prev_epochs\n",
    "          \n",
    "          if mean_loss > (prev_mean_loss - 0.01*prev_mean_loss):\n",
    "            # break\n",
    "            last_epoch = True\n",
    "\n",
    "        tr_ls.append(tmp_tr_loss / tr_sample)\n",
    "\n",
    "        if not os.path.exists('./current experiment/Train_Output/' + DIREC):\n",
    "            os.makedirs('./current experiment/Train_Output/' + DIREC)\n",
    "\n",
    "\n",
    "        score_model.eval()\n",
    "        if last_epoch:\n",
    "            last_epoch_num = epoch+1\n",
    "            with torch.no_grad():\n",
    "                for idx, (data, target) in enumerate(test_loader):\n",
    "                        condition = data.to(device)\n",
    "\n",
    "                        tic_od = time.time()\n",
    "                        samples1 = sampler_od(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "                        toc_od = time.time()\n",
    "                        time_interval_od = toc_od - tic_od\n",
    "                        od_sum_time += time_interval_od\n",
    "\n",
    "                        tic_eu = time.time()\n",
    "                        samples2 = sampler_eu(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "                        toc_eu = time.time()\n",
    "                        time_interval_eu = toc_eu - tic_eu\n",
    "                        eu_sum_time += time_interval_eu\n",
    "\n",
    "                        tic_pc = time.time()\n",
    "                        samples3 = sampler_pc(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "                        toc_pc = time.time()\n",
    "                        time_interval_pc = toc_pc - tic_pc\n",
    "                        pc_sum_time += time_interval_pc\n",
    "\n",
    "                        diff_out_od = np.array(samples1.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_od_number_{idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_od)\n",
    "\n",
    "                        diff_out_eu = np.array(samples2.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_eu_number_{idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_eu)\n",
    "\n",
    "                        diff_out_pc = np.array(samples3.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_pc_number_{idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_pc)\n",
    "                        # sample visulization\n",
    "                        samples1 = samples1.clamp(0., 1.)\n",
    "                        samples2 = samples2.clamp(0., 1.)\n",
    "                        samples3 = samples3.clamp(0., 1.)\n",
    "\n",
    "                        fig = plt.figure()\n",
    "                        fig.set_figheight(4)\n",
    "                        fig.set_figwidth(20)\n",
    "                        spec = gridspec.GridSpec(ncols=5, nrows=1,\n",
    "                                              width_ratios=[1,1,1,1,1], wspace=0.01,\n",
    "                                              hspace=0.01, height_ratios=[1],left=0,right=1,top=1,bottom=0)\n",
    "                        ax = fig.add_subplot(spec[0])\n",
    "                        ax.imshow(data[0].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[1])\n",
    "                        ax.imshow(samples1[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[2])\n",
    "                        ax.imshow(samples2[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[3])\n",
    "                        ax.imshow(samples3[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[4])\n",
    "                        ax.imshow(target[0].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "\n",
    "\n",
    "                        plt.savefig('./current experiment/Train_Output/'+ DIREC + '/Epoch_' + str(epoch+1) + '.png',\n",
    "                                    bbox_inches='tight', pad_inches=0)\n",
    "                        \n",
    "\n",
    "                break\n",
    "\n",
    "    return od_sum_time, eu_sum_time, pc_sum_time, last_epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_sum_time, eu_sum_time, pc_sum_time, last_epoch_num = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_avg_time = od_sum_time / (n_test_new * last_epoch_num)\n",
    "eu_avg_time = eu_sum_time / (n_test_new * last_epoch_num)\n",
    "pc_avg_time = pc_sum_time / (n_test_new * last_epoch_num)\n",
    "\n",
    "od_avg_time, eu_avg_time, pc_avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_outs = [None] * n_test_new\n",
    "sampler = 'pc'\n",
    "last_epoch_num = 102\n",
    "img_size = 128\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  diff_out = np.load(f'./current experiment/diff_results/x0_{sampler}_number_{i+1}_epoch_{last_epoch_num}.npy')\n",
    "  diff_out = np.reshape(diff_out, (1, img_size, img_size))\n",
    "  diff_out = torch.tensor(diff_out)\n",
    "  diff_outs[i] = diff_out.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import PSNR, SSIM\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "\n",
    "# create default evaluator for doctests\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_evaluator = Engine(eval_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [None] * n_test_new\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  ct_sample = ct_test_resized[i]\n",
    "  ct_sample = np.reshape(ct_sample, (1, img_size, img_size))\n",
    "  ct_sample = torch.tensor(ct_sample)\n",
    "  targets[i] = ct_sample.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_out_sample = diff_outs[0]\n",
    "targets_sample = targets[0]\n",
    "\n",
    "(diff_out_sample.dtype), (targets_sample.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = SSIM(data_range=1.0)\n",
    "metric.attach(default_evaluator, 'ssim')\n",
    "\n",
    "\n",
    "sum_ssims = 0\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  state = default_evaluator.run([[diff_outs[i].float(), targets[i].float()]])\n",
    "  ssim_value = state.metrics['ssim']\n",
    "  # print(ssim_value)\n",
    "  sum_ssims += ssim_value\n",
    "\n",
    "avg_ssim = sum_ssims / n_test_new\n",
    "\n",
    "avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PSNR(data_range=1.0)\n",
    "metric.attach(default_evaluator, 'psnr')\n",
    "\n",
    "\n",
    "sum_psnrs = 0\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  state = default_evaluator.run([[diff_outs[i].float(), targets[i].float()]])\n",
    "  psnr_value = state.metrics['psnr']\n",
    "  # print(ssim_value)\n",
    "  sum_psnrs += psnr_value\n",
    "\n",
    "avg_psnr = sum_psnrs / n_test_new\n",
    "\n",
    "avg_psnr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
