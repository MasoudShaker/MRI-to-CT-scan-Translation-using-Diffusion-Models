{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((570, 256, 256), (90, 256, 256), (150, 256, 256))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/home/iplab/Desktop/Shaker/Brain tumor MRI and CT scan/data(processed)'\n",
    "\n",
    "# read mr and ct datasets\n",
    "MR_train_address = os.path.join(base_dir, 'train_input.npy')\n",
    "CT_train_address = os.path.join(base_dir, 'train_output.npy')\n",
    "\n",
    "MR_val_address = os.path.join(base_dir, 'val_input.npy')\n",
    "CT_val_address = os.path.join(base_dir, 'val_output.npy')\n",
    "\n",
    "MR_test_address = os.path.join(base_dir, 'test_input.npy')\n",
    "CT_test_address = os.path.join(base_dir, 'test_output.npy')\n",
    "\n",
    "mr_train = np.load(MR_train_address)\n",
    "ct_train = np.load(CT_train_address)\n",
    "\n",
    "mr_val = np.load(MR_val_address)\n",
    "ct_val = np.load(CT_val_address)\n",
    "\n",
    "mr_test = np.load(MR_test_address)\n",
    "ct_test = np.load(CT_test_address)\n",
    "\n",
    "ct_train.shape, ct_val.shape, ct_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes a 1d numpy array to an arbitrary size\n",
    "def resize(img, size):\n",
    "\n",
    "  img = img.astype('float32')\n",
    "  img = torch.tensor(img)\n",
    "  img = img.unsqueeze(0)\n",
    "\n",
    "  transform = T.Resize(size)\n",
    "  resized_img = transform(img)\n",
    "\n",
    "  # resized_img = np.array(resized_img)\n",
    "\n",
    "  return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_gray_level = T.Compose([\n",
    "    T.Lambda(lambda t: t * 0.2)\n",
    "])\n",
    "\n",
    "horizontal_flip = T.Compose([\n",
    "    T.functional.hflip\n",
    "])\n",
    "\n",
    "vertical_flip = T.Compose([\n",
    "    T.functional.vflip\n",
    "])\n",
    "\n",
    "rotate_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=45))\n",
    "])\n",
    "\n",
    "rotate_minus_45 = T.Compose([\n",
    "    T.Lambda(lambda t: T.functional.rotate(t, angle=-45))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iplab/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_train_samples = ct_train.shape[0]\n",
    "n_val_samples = ct_val.shape[0]\n",
    "n_test_samples = ct_test.shape[0]\n",
    "\n",
    "# remove 90 samples from test data and add it to train data\n",
    "n_add_from_test_to_train = 69\n",
    "\n",
    "n_train_new = n_train_samples + n_val_samples + n_add_from_test_to_train\n",
    "n_test_new = n_test_samples - n_add_from_test_to_train\n",
    "\n",
    "# data augmentation\n",
    "# add 5 varient of each sample\n",
    "# so our train dataset will be 6 times bigger\n",
    "\n",
    "n_train_new = n_train_new * 6\n",
    "# n_train_new = n_train_new * 3\n",
    "\n",
    "mr_train_resized = [None] * n_train_new\n",
    "ct_train_resized = [None] * n_train_new\n",
    "\n",
    "mr_test_resized = [None] * n_test_new\n",
    "ct_test_resized = [None] * n_test_new\n",
    "\n",
    "# train samples with augmentation\n",
    "for i in range(n_train_samples):\n",
    "\n",
    "  j = i * 6\n",
    "  # j = i*3\n",
    "  # j = i\n",
    "\n",
    "  resized_mr = resize(mr_train[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_train[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "\n",
    "# validation samples with augmentation\n",
    "for i in range(n_val_samples):\n",
    "\n",
    "  j = i*6 + n_train_samples*6\n",
    "  # j = i*3 + n_train_samples*3\n",
    "  # j = i + n_train_samples\n",
    "\n",
    "  resized_mr = resize(mr_val[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_val[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "\n",
    "# part of test samples with augmentation\n",
    "\n",
    "for i in range(n_add_from_test_to_train):\n",
    "\n",
    "  j = i*6 + n_train_samples*6 + n_val_samples*6\n",
    "  # j = i*3 + n_train_samples*3 + n_val_samples*3\n",
    "  # j = i + n_train_samples + n_val_samples\n",
    "\n",
    "  resized_mr = resize(mr_test[i], 128)\n",
    "\n",
    "  mr_train_resized[j] = resized_mr\n",
    "  mr_train_resized[j+1] = change_gray_level(resized_mr)\n",
    "  mr_train_resized[j+2] = horizontal_flip(resized_mr)\n",
    "  mr_train_resized[j+3] = vertical_flip(resized_mr)\n",
    "  mr_train_resized[j+4] = rotate_45(resized_mr)\n",
    "  mr_train_resized[j+5] = rotate_minus_45(resized_mr)\n",
    "\n",
    "  resized_ct = resize(ct_test[i], 128)\n",
    "\n",
    "  ct_train_resized[j] = resized_ct\n",
    "  ct_train_resized[j+1] = change_gray_level(resized_ct)\n",
    "  ct_train_resized[j+2] = horizontal_flip(resized_ct)\n",
    "  ct_train_resized[j+3] = vertical_flip(resized_ct)\n",
    "  ct_train_resized[j+4] = rotate_45(resized_ct)\n",
    "  ct_train_resized[j+5] = rotate_minus_45(resized_ct)\n",
    "\n",
    "# test samples\n",
    "for i in range(n_test_new):\n",
    "\n",
    "  j = i + n_add_from_test_to_train\n",
    "\n",
    "  mr_test_resized[i] = resize(mr_test[j], 128)\n",
    "  ct_test_resized[i] = resize(ct_test[j], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test samples to numpy array\n",
    "for i in range(n_train_new):\n",
    "  mr_train_resized[i] = np.array(mr_train_resized[i].squeeze())\n",
    "  ct_train_resized[i] = np.array(ct_train_resized[i].squeeze())\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  mr_test_resized[i] = np.array(mr_test_resized[i].squeeze())\n",
    "  ct_test_resized[i] = np.array(ct_test_resized[i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape: (4374, 128, 128)\n",
      "test images shape: (81, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# convert lists of mr and ct to numpy arrays\n",
    "mr_train_resized = np.array(mr_train_resized)\n",
    "\n",
    "ct_train_resized = np.array(ct_train_resized)\n",
    "\n",
    "mr_test_resized = np.array(mr_test_resized)\n",
    "\n",
    "ct_test_resized = np.array(ct_test_resized)\n",
    "\n",
    "\n",
    "print('train images shape:', mr_train_resized.shape)\n",
    "print('test images shape:', mr_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# creating hdf5 data from numpy arrays\n",
    "with h5py.File('mr_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_train_resized)\n",
    "\n",
    "with h5py.File('ct_train_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_train_resized)\n",
    "\n",
    "with h5py.File('mr_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = mr_test_resized)\n",
    "\n",
    "with h5py.File('ct_test_resized.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"data\", data = ct_test_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data\": shape (4374, 128, 128), type \"<f4\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'mr_train_resized.hdf5'\n",
    "f = h5py.File(path,'r')\n",
    "load_data = f['data']\n",
    "load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "def random_rot(img1,img2):\n",
    "    k = np.random.randint(0, 3)\n",
    "    img1 = np.rot90(img1, k+1)\n",
    "    img2 = np.rot90(img2, k+1)\n",
    "    return img1,img2\n",
    "\n",
    "def random_flip(img1,img2):\n",
    "    axis = np.random.randint(0, 2)\n",
    "    img1 = np.flip(img1, axis=axis).copy()\n",
    "    img2 = np.flip(img2, axis=axis).copy()\n",
    "    return img1,img2\n",
    "\n",
    "class RandomGenerator(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        lr, hr = sample['lr'], sample['hr']\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = random_rot(lr, hr)\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = random_flip(lr, hr)\n",
    "        sample = {'lr': lr,'hr': hr}\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class Train_Data(Dataset):\n",
    "    def __init__(self):\n",
    "        path = 'mr_train_resized.hdf5' # data in hdf5 as an example\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.lr = load_data\n",
    "        path = 'ct_train_resized.hdf5'\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.hr = load_data\n",
    "        c, self.h, self.w = self.lr.shape\n",
    "\n",
    "        self.len = c\n",
    "        self.transform=transforms.Compose([RandomGenerator(output_size=[self.h, self.w])])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.lr[index, :, :]\n",
    "        y = self.hr[index, :, :]\n",
    "\n",
    "        x = self.norm(x)\n",
    "        y = self.norm(y)\n",
    "\n",
    "        sample = {'lr': x,'hr': y}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        x, y = sample['lr'], sample['hr']\n",
    "\n",
    "        xx = np.zeros((1, self.h, self.w))\n",
    "        yy = np.zeros((1, self.h, self.w))\n",
    "\n",
    "        xx[0,:,:] = x.copy()\n",
    "        yy[0,:,:] = y.copy()\n",
    "\n",
    "        xx = torch.from_numpy(xx)\n",
    "        yy = torch.from_numpy(yy)\n",
    "\n",
    "        xx = xx.type(torch.FloatTensor)\n",
    "        yy = yy.type(torch.FloatTensor)\n",
    "\n",
    "        return xx, yy\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def norm(self, x):\n",
    "        if np.amax(x) > 0:\n",
    "            x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Valid_Data(Dataset):\n",
    "    def __init__(self):\n",
    "        path = 'mr_test_resized.hdf5'\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.lr = load_data\n",
    "        path = 'ct_test_resized.hdf5'\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.hr = load_data\n",
    "        c, self.h, self.w = self.lr.shape\n",
    "\n",
    "        self.len = c\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.lr[index, :, :]\n",
    "        y = self.hr[index, :, :]\n",
    "\n",
    "        x = self.norm(x)\n",
    "        y = self.norm(y)\n",
    "\n",
    "        xx = np.zeros((1, self.h, self.w))\n",
    "        yy = np.zeros((1, self.h, self.w))\n",
    "\n",
    "        xx[0,:,:] = x.copy()\n",
    "        yy[0,:,:] = y.copy()\n",
    "\n",
    "        xx = torch.from_numpy(xx)\n",
    "        yy = torch.from_numpy(yy)\n",
    "\n",
    "        xx = xx.type(torch.FloatTensor)\n",
    "        yy = yy.type(torch.FloatTensor)\n",
    "\n",
    "        return xx, yy\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def norm(self, x):\n",
    "        if np.amax(x) > 0:\n",
    "            x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
    "        return x\n",
    "\n",
    "class Test_Data(Dataset):\n",
    "    def __init__(self):\n",
    "        path = '/path/to/test_mri_data'\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.lr = load_data\n",
    "        path = '/path/to/test_ct_data'\n",
    "        f = h5py.File(path,'r')\n",
    "        load_data = f['data']\n",
    "        self.hr = load_data\n",
    "        c, self.h, self.w = self.lr.shape\n",
    "\n",
    "        self.len = 135\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.lr[index, :, :]\n",
    "        y = self.hr[index, :, :]\n",
    "\n",
    "        x = self.norm(x)\n",
    "        y = self.norm(y)\n",
    "\n",
    "        xx = np.zeros((1, self.h, self.w))\n",
    "        yy = np.zeros((1, self.h, self.w))\n",
    "\n",
    "        xx[0,:,:] = x.copy()\n",
    "        yy[0,:,:] = y.copy()\n",
    "\n",
    "        xx = torch.from_numpy(xx)\n",
    "        yy = torch.from_numpy(yy)\n",
    "\n",
    "        xx = xx.type(torch.FloatTensor)\n",
    "        yy = yy.type(torch.FloatTensor)\n",
    "\n",
    "        return xx, yy\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def norm(self, x):\n",
    "        if np.amax(x) > 0:\n",
    "            x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from scipy import integrate\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "    \"\"\" Compute standard deviation of conditional Gaussian distribution at time t.\n",
    "    SDE: dx = sigma^t * dw  t belongs to [0,1]\n",
    "    p_{0t}(x(t) | x(0)) = N(m,var) = N(x(t); x(0), 1 / (2log(sigma)) * (sigma^(2t)-1) * I)\n",
    "\n",
    "    \"\"\"\n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    \"\"\" Compute diffusion coefficient at time t \"\"\"\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "def loss_fn(score_model, condition, x, marginal_prob_std, eps=1e-5):\n",
    "    \"\"\" The loss function for training score-based generative models\n",
    "\n",
    "    Args:\n",
    "        score_model: A PyTorch model instance that represents a time-dependent\n",
    "            score-based model\n",
    "        x: A mini-batch of training data\n",
    "        condition: input image as condition\n",
    "        marginal_prob_std: A function that gives the standard deviation of\n",
    "            the perturbation kernel\n",
    "        eps: A tolerance value for numerical stability\n",
    "    \"\"\"\n",
    "    # Step 1: randomly generate time t from [0.0001, 0.9999]\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "\n",
    "    # Step 2: sampling a perturbed_x sample from data distribtion p_t(x) based on the reparameterization trick\n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)\n",
    "    perturbed_x = x + z * std[:, None, None, None]\n",
    "\n",
    "    # Step 3: adding noised sample into the score network to estimate score\n",
    "    score = score_model(torch.cat([perturbed_x, condition], dim=1), random_t)\n",
    "\n",
    "    # Step 4: Computing score matching loss\n",
    "    loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        super(EMA, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        # self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "\n",
    "\n",
    "def euler_sampler(score_model, condition, marginal_prob_std, diffusion_coeff, batch_size=64, num_steps=1000, eps=1e-3):\n",
    "    \"\"\"\n",
    "       SDE: dx = f(x,t)dt + g(t)dw\n",
    "       Reverse-time SDE: dx = [f(x,t) - g(t)**2 * score]dt + g(t)dw_bar\n",
    "       In this case, omit f(x,t) and choose SDE: dx = sigma*t * dw  t belongs to [0,1]\n",
    "       Reverse-time SDE: dx = -sigma**{2t} * score * dt + sigma**t * dw_bar\n",
    "\n",
    "       To sample from time-dependent score-based model, first draw a sample from the prior\n",
    "       distribution p_1 ~ N(x; 0, 0.5*(sigma**2 - 1)*I), then solve the reverse-time SDE\n",
    "       via Euler-Maruyama approach. Replacing dw with z ~ N(0, g(t)**2 * dt * I),\n",
    "       we can obtain the iteration rule:\n",
    "           x_{t-dt} = x_t + sigma**{2t} * score * dt + sigma**t * sqrt(dt) * z_t, where\n",
    "           z_t ~ N(0,I)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: define start time t=1 and random samples from prior data distribution\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, 1, 128, 128, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "\n",
    "    # Step 2: define reverse time grid and time invervals\n",
    "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "\n",
    "    # Step 3: solve reverse time SDE via Euler-Maruyama approach\n",
    "    x = init_x\n",
    "    with torch.no_grad():\n",
    "        for time_step in time_steps:\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            mean_x = x + (g**2)[:, None, None, None] * score_model(torch.cat([x, condition], dim=1), batch_time_step) * step_size\n",
    "            x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
    "\n",
    "    # Step 4: select final step expectation as a sampler\n",
    "    return mean_x\n",
    "\n",
    "\n",
    "def pc_sampler(score_model, condition, marginal_prob_std, diffusion_coeff, batch_size=64, snr=0.16, num_steps=1000, eps=1e-3):\n",
    "    \"\"\" Generate samplers from score-based models with Predictor-Corrector method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    score_model : A PyTorch model instance that represents a time-dependent\n",
    "            score-based model.\n",
    "    marginal_prob_std : A function that gives the standard deviation of\n",
    "            the perturbation kernel\n",
    "    diffusion_coeff : A function that gives the diffusion coefficient\n",
    "    batch_size : default: 64\n",
    "    snr : signal-to-noise-ratio, default: 0.16\n",
    "    \"\"\"\n",
    "    # Step 1: define start time t=1 and random samples from prior data distribution\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, 1, 128, 128, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "\n",
    "    # Step 2: define reverse time grid and time invervals\n",
    "    time_steps = np.linspace(1., eps, num_steps)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "\n",
    "    # Step 3: alternatively use Langevin sampling and reverse-time SDE with Euler approach to solve\n",
    "    x = init_x\n",
    "    with torch.no_grad():\n",
    "        for time_step in time_steps:\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "\n",
    "            # Corrector step (Langevin MCMC)\n",
    "            grad = score_model(torch.cat([x, condition], dim=1), batch_time_step)\n",
    "            grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "            noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "            langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
    "            # print(f\"{langevin_step_size=}\")\n",
    "\n",
    "            for _ in range(10):\n",
    "                x = x + langevin_step_size * grad + torch.sqrt(2 * langevin_step_size) * torch.randn_like(x)\n",
    "                grad = score_model(torch.cat([x, condition], dim=1), batch_time_step)\n",
    "                grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "                noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "                langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
    "                # print(f\"{langevin_step_size=}\")\n",
    "\n",
    "            # Predictor step (Euler-Maruyama)\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            mean_x = x + (g**2)[:, None, None, None] * score_model(torch.cat([x, condition], dim=1), batch_time_step) * step_size\n",
    "            x = mean_x + torch.sqrt(g**2 * step_size)[:, None, None, None] * torch.randn_like(x)\n",
    "\n",
    "        # Step 4: select final step expectation as a sampler\n",
    "        return mean_x\n",
    "\n",
    "def ode_sampler(score_model, condition, marginal_prob_std, diffusion_coeff, batch_size=64, atol=1e-5, rtol=1e-5, z=None, eps=1e-3):\n",
    "    \"\"\" Generate samplers from score-based models with ODE method \"\"\"\n",
    "\n",
    "    # Step 1: define start time t=1 and initial x\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    if z is None:\n",
    "        init_x = torch.randn(batch_size, 1, 128, 128, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "    else:\n",
    "        init_x = z\n",
    "    shape = init_x.shape\n",
    "\n",
    "    # Step 2: define score estimation function and ODE function\n",
    "    def score_eval_wrapper(sample, time_steps):\n",
    "        \"\"\" A Wrapper of the score-based model for use by the ODE solver \"\"\"\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "        with torch.no_grad():\n",
    "            score = score_model(torch.cat([sample, condition], dim=1), time_steps)\n",
    "        return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "\n",
    "    def ode_func(t, x):\n",
    "        \"\"\" The ODE function for use by the ODE solver \"\"\"\n",
    "        time_steps = np.ones((shape[0],)) * t\n",
    "        g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
    "        return -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "\n",
    "    # Step 3: using ODE to solve value at t=eps\n",
    "    res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "    # print(f\"Number of function evaluations: {res.nfev}\")\n",
    "\n",
    "    x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights druing initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :].to(x.device) * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = F.interpolate(\n",
    "            x, scale_factor=2, mode='nearest')\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.block1(x) # [batch, out_ch, h, w]\n",
    "        h += self.temb_proj(temb)[:, :, None, None] # [batch, out_ch, :, :]\n",
    "        h = self.block2(h) # [batch, out_ch, h, w]\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, marginal_prob_std, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(tdim)\n",
    "        # self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "\n",
    "        self.head = nn.Conv2d(2, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(\n",
    "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=False))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(\n",
    "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=False))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 1, 3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.head.weight)\n",
    "        init.zeros_(self.head.bias)\n",
    "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
    "        init.zeros_(self.tail[-1].bias)\n",
    "\n",
    "    def forward(self, x, t):  # t [batch,], x [batch,3,h,w]\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t) # [batch, 128*4]\n",
    "        # Downsampling\n",
    "        h = self.head(x) # [batch,128,h,w]\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks: # [res, res, down; res, res, down; res, res, down; res, res]\n",
    "            h = layer(h, temb)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks: # [res, res]\n",
    "            h = layer(h, temb)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks: # [res,res,res; res,res,res,up; res,res,res,up; res,res,res,up]\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb)\n",
    "        h = self.tail(h)\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None] # divide the expectation of L2-norm\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 24.91 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2187 [00:00<?, ?batch/s]<ipython-input-12-2d89901161cc>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "Epoch 1: 100%|██████████| 2187/2187 [02:24<00:00, 15.16batch/s, Loss=28.6]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 247.17520786345577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2187/2187 [02:21<00:00, 15.48batch/s, Loss=100]    \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 51.683993379438526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2187/2187 [02:20<00:00, 15.52batch/s, Loss=72.4]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 46.86444416076667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2187/2187 [02:21<00:00, 15.48batch/s, Loss=38.7]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 40.58279460633133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2187/2187 [02:20<00:00, 15.54batch/s, Loss=65]     \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 34.694274219193304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2187/2187 [02:21<00:00, 15.50batch/s, Loss=194]    \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 32.80211564335694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2187/2187 [02:21<00:00, 15.49batch/s, Loss=3.7]    \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 27.913708628395895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2187/2187 [02:20<00:00, 15.53batch/s, Loss=34.7]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 27.569816202129683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2187/2187 [02:21<00:00, 15.44batch/s, Loss=46.3]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 27.386405454959615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2187/2187 [02:21<00:00, 15.41batch/s, Loss=12]     \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 26.150790516295604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2187/2187 [02:21<00:00, 15.47batch/s, Loss=13.6]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 26.046797305577194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2187/2187 [02:22<00:00, 15.37batch/s, Loss=16.6]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 23.393867710217744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2187/2187 [02:22<00:00, 15.39batch/s, Loss=16.8]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 23.099694393571784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2187/2187 [02:21<00:00, 15.42batch/s, Loss=54]     \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 23.04272699852103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2187/2187 [02:22<00:00, 15.38batch/s, Loss=56.5]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 21.28134956518482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2187/2187 [02:21<00:00, 15.43batch/s, Loss=111]    \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 21.629968897060113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2187/2187 [02:21<00:00, 15.49batch/s, Loss=27.3]   \n",
      "  0%|          | 0/2187 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 21.807750146357392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:  26%|██▌       | 569/2187 [00:39<01:51, 14.53batch/s, Loss=138] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e709fee40872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e709fee40872>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mx_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarginal_prob_std_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mtmp_tr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-2d89901161cc>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(score_model, condition, x, marginal_prob_std, eps)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Step 3: adding noised sample into the score network to estimate score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperturbed_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Step 4: Computing score matching loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6c54ae1e6934>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupblocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [res,res,res; res,res,res,up; res,res,res,up; res,res,res,up]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import os\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "# from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train = True\n",
    "continue_train = False\n",
    "\n",
    "# UNet\n",
    "ch = 64\n",
    "ch_mult = [1, 2, 2, 4, 4]\n",
    "attn = [1]\n",
    "num_res_blocks = 2\n",
    "dropout = 0.\n",
    "\n",
    "# Gaussian Diffusion\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Training\n",
    "lr = 1e-4\n",
    "grad_clip = 1.\n",
    "img_size = 128\n",
    "batch_size = 2\n",
    "num_workers = 1\n",
    "ema_decay = 0.9999\n",
    "\n",
    "sample_size = 1\n",
    "\n",
    "min_epoch = 5\n",
    "max_epoch = 5000\n",
    "\n",
    "epoch_mean_loss = max_epoch * [None]\n",
    "n_prev_epochs = 5\n",
    "\n",
    "DIREC = f'score-unet_n-train-samples_{n_train_new}n-test-samples_{n_test_new}_batch-size_{batch_size}_T_{T}_img-size_{img_size}_data_augmentation_all'\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def train():\n",
    "    sigma = 25.\n",
    "    marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma) # construc function without parameters\n",
    "    diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma) # construc function without parameters\n",
    "\n",
    "    # dataset\n",
    "    tr_train = Train_Data()\n",
    "    trainloader = DataLoader(tr_train, batch_size=batch_size, num_workers=num_workers,\n",
    "                             pin_memory=True, shuffle=True)\n",
    "    va_train = Valid_Data()\n",
    "    validloader = DataLoader(va_train, batch_size=sample_size, num_workers=num_workers,\n",
    "                             pin_memory=True, shuffle=False)\n",
    "\n",
    "    # model setup\n",
    "    score_model = UNet(marginal_prob_std=marginal_prob_std_fn,\n",
    "                       T=T, ch=ch, ch_mult=ch_mult, attn=attn,\n",
    "                       num_res_blocks=num_res_blocks, dropout=dropout).to(device)\n",
    "\n",
    "    ema_model = EMA(score_model).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "    # sampler setup\n",
    "    sampler_od = ode_sampler\n",
    "    sampler_eu = euler_sampler\n",
    "    sampler_pc = pc_sampler\n",
    "\n",
    "    # show model size\n",
    "    model_size = 0\n",
    "    for param in score_model.parameters():\n",
    "        model_size += param.data.nelement()\n",
    "    print('Model params: %.2f M' % (model_size / 1024 / 1024))\n",
    "\n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./Save/' + DIREC + '/model_latest.pkl')\n",
    "        score_model.load_state_dict(checkpoint['score_model'])\n",
    "        ema_model.load_state_dict(checkpoint['ema_model'])\n",
    "        optim.load_state_dict(checkpoint['optim'])\n",
    "        restore_epoch = checkpoint['epoch']\n",
    "        print('Finish loading model')\n",
    "    else:\n",
    "        restore_epoch = 0\n",
    "\n",
    "    # if not os.path.exists('Loss'):\n",
    "    #     os.makedirs('Loss')\n",
    "    if not os.path.exists('current experiment'):\n",
    "        os.makedirs('current experiment')\n",
    "\n",
    "    tr_ls = []\n",
    "    if continue_train:\n",
    "        readmat = sio.loadmat('./Loss/' + DIREC)\n",
    "        load_tr_ls = readmat['loss']\n",
    "        for i in range(restore_epoch):\n",
    "            tr_ls.append(load_tr_ls[0][i])\n",
    "        print('Finish loading loss!')\n",
    "\n",
    "\n",
    "    if not os.path.exists('./current experiment/diff_results'):\n",
    "        os.makedirs('./current experiment/diff_results')\n",
    "\n",
    "    last_epoch = False\n",
    "\n",
    "    for epoch in range(restore_epoch, max_epoch):\n",
    "        with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "            tmp_tr_loss = 0\n",
    "            tr_sample = 0\n",
    "            score_model.train()\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "                # train\n",
    "                condition = data.to(device)\n",
    "                x_0 = target.to(device)\n",
    "\n",
    "                loss = loss_fn(score_model, condition, x_0, marginal_prob_std_fn)\n",
    "\n",
    "                tmp_tr_loss += loss.item()\n",
    "                tr_sample += len(data)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                ema_model.update(score_model)\n",
    "\n",
    "                tepoch.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        mean_loss = tmp_tr_loss / tr_sample\n",
    "        print('mean loss', mean_loss)\n",
    "\n",
    "        epoch_mean_loss[epoch] = mean_loss\n",
    "        \n",
    "        if epoch+1 > min_epoch:\n",
    "          prev_mean_loss = 0\n",
    "          \n",
    "          for i in range(n_prev_epochs):\n",
    "            prev_mean_loss += epoch_mean_loss[epoch - (i+1)]\n",
    "\n",
    "          prev_mean_loss /= n_prev_epochs\n",
    "          \n",
    "          if mean_loss > (prev_mean_loss - 0.01*prev_mean_loss):\n",
    "            # break\n",
    "            last_epoch = True\n",
    "\n",
    "        tr_ls.append(tmp_tr_loss / tr_sample)\n",
    "        # sio.savemat('./Loss/' + DIREC +'.mat', {'loss': tr_ls})\n",
    "\n",
    "        if not os.path.exists('./current experiment/Train_Output/' + DIREC):\n",
    "            os.makedirs('./current experiment/Train_Output/' + DIREC)\n",
    "\n",
    "\n",
    "        score_model.eval()\n",
    "        if last_epoch:\n",
    "        # if (epoch+1) >= 100 and ((epoch+1) % 5 == 0):\n",
    "        # if epoch >= 0:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(validloader):\n",
    "                    if batch_idx == 0:\n",
    "                        condition = data.to(device)\n",
    "\n",
    "                        samples1 = sampler_od(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "                        samples2 = sampler_eu(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "                        samples3 = sampler_pc(score_model, condition, marginal_prob_std_fn, diffusion_coeff_fn, sample_size)\n",
    "\n",
    "                        diff_out_od = np.array(samples1.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_od_number_{batch_idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_od)\n",
    "\n",
    "                        diff_out_eu = np.array(samples2.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_eu_number_{batch_idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_eu)\n",
    "\n",
    "                        diff_out_pc = np.array(samples3.cpu())\n",
    "                        save_path = f'./current experiment/diff_results/x0_pc_number_{batch_idx+1}_epoch_{epoch+1}.npy'\n",
    "                        np.save(save_path, diff_out_pc)\n",
    "                        # sample visulization\n",
    "                        samples1 = samples1.clamp(0., 1.)\n",
    "                        samples2 = samples2.clamp(0., 1.)\n",
    "                        samples3 = samples3.clamp(0., 1.)\n",
    "\n",
    "                        fig = plt.figure()\n",
    "                        # fig.set_figheight(8)\n",
    "                        fig.set_figheight(4)\n",
    "                        fig.set_figwidth(20)\n",
    "                        spec = gridspec.GridSpec(ncols=5, nrows=1,\n",
    "                                              width_ratios=[1,1,1,1,1], wspace=0.01,\n",
    "                                              hspace=0.01, height_ratios=[1],left=0,right=1,top=1,bottom=0)\n",
    "                        ax = fig.add_subplot(spec[0])\n",
    "                        ax.imshow(data[0].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[1])\n",
    "                        ax.imshow(samples1[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[2])\n",
    "                        ax.imshow(samples2[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[3])\n",
    "                        ax.imshow(samples3[0].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "                        ax = fig.add_subplot(spec[4])\n",
    "                        ax.imshow(target[0].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        ax.axis('off')\n",
    "\n",
    "                        # ax = fig.add_subplot(spec[5])\n",
    "                        # ax.imshow(data[1].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        # ax.axis('off')\n",
    "                        # ax = fig.add_subplot(spec[6])\n",
    "                        # ax.imshow(samples1[1].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        # ax.axis('off')\n",
    "                        # ax = fig.add_subplot(spec[7])\n",
    "                        # ax.imshow(samples2[1].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        # ax.axis('off')\n",
    "                        # ax = fig.add_subplot(spec[8])\n",
    "                        # ax.imshow(samples3[1].squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        # ax.axis('off')\n",
    "                        # ax = fig.add_subplot(spec[9])\n",
    "                        # ax.imshow(target[1].data.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                        # ax.axis('off')\n",
    "\n",
    "                        plt.savefig('./current experiment/Train_Output/'+ DIREC + '/Epoch_' + str(epoch+1) + '.png',\n",
    "                                    bbox_inches='tight', pad_inches=0)\n",
    "                        \n",
    "                break\n",
    "\n",
    "        # # save\n",
    "        # if not os.path.exists('Save/' + DIREC):\n",
    "        #     os.makedirs('Save/' + DIREC)\n",
    "        # ckpt = {\n",
    "        #     'score_model': score_model.state_dict(),\n",
    "        #     'ema_model': ema_model.state_dict(),\n",
    "        #     'optim': optim.state_dict(),\n",
    "        #     'epoch': epoch+1,\n",
    "        # }\n",
    "        # if (epoch+1) % 20 == 0:\n",
    "        #     torch.save(ckpt, './Save/' + DIREC + '/model_epoch_'+str(epoch+1)+'.pkl')\n",
    "        # torch.save(ckpt, './Save/' + DIREC + '/model_latest.pkl')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "diff_outs = [None] * n_test_new\n",
    "sampler = 'od'\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  diff_out = np.load(f'./current experiment/diff_results/x0_{sampler}_number_{batch_idx+1}_epoch_{epoch+1}.npy')\n",
    "  diff_out = np.reshape(diff_out, (1, img_size, img_size))\n",
    "  diff_out = torch.tensor(diff_out)\n",
    "  diff_outs[i] = diff_out.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import PSNR, SSIM\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "\n",
    "# create default evaluator for doctests\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_evaluator = Engine(eval_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [None] * n_test_new\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  ct_sample = ct_test_resized[i]\n",
    "  ct_sample = np.reshape(ct_sample, (1, img_size, img_size))\n",
    "  ct_sample = torch.tensor(ct_sample)\n",
    "  targets[i] = ct_sample.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = SSIM(data_range=1.0)\n",
    "metric.attach(default_evaluator, 'ssim')\n",
    "\n",
    "\n",
    "ssims_epoch_80 = [None] * n_test_new\n",
    "sum_ssims = 0\n",
    "\n",
    "for i in range(n_test_new):\n",
    "  state = default_evaluator.run([[diff_outs[i], targets[i]]])\n",
    "  ssim_value = state.metrics['ssim']\n",
    "  # print(ssim_value)\n",
    "  sum_ssims += ssim_value\n",
    "\n",
    "avg_ssim = sum_ssims / n_test_new\n",
    "\n",
    "avg_ssim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
